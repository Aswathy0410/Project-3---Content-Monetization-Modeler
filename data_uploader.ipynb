{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "320b078a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import streamlit as st\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for plots\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"🚀 YouTube Monetization Modeler - Starting Analysis\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Step 1: Load and Understand the Dataset\n",
    "def load_data():\n",
    "    \"\"\"Load the YouTube dataset\"\"\"\n",
    "    # For demonstration, I'll create a sample dataset based on your description\n",
    "    # In practice, replace this with: df = pd.read_csv('youtube_data.csv')\n",
    "    \n",
    "    # Create sample data based on your description\n",
    "    np.random.seed(42)\n",
    "    n_samples = 122400\n",
    "   \n",
    "    data= (\"F:/MDTM46B/Project 3/Content Monetization Modeler/youtube_ad_revenue_dataset.csv\")\n",
    "    df=pd.read_csv(data)\n",
    "    \n",
    "    # Add some missing values (5%) and duplicates (2%) as per requirements\n",
    "    mask = np.random.rand(n_samples) < 0.05\n",
    "    df.loc[mask, 'likes'] = np.nan\n",
    "    df.loc[mask, 'comments'] = np.nan\n",
    "    \n",
    "    # Add some duplicates\n",
    "    dup_mask = np.random.choice(n_samples, size=int(0.02 * n_samples), replace=False)\n",
    "    df = pd.concat([df, df.iloc[dup_mask]], ignore_index=True)\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Load data\n",
    "df = load_data()\n",
    "print(f\"📊 Dataset loaded: {df.shape[0]} rows, {df.shape[1]} columns\")\n",
    "print(\"\\nFirst 5 rows:\")\n",
    "print(df.head())\n",
    "\n",
    "print(\"\\nDataset Info:\")\n",
    "print(df.info())\n",
    "\n",
    "# Step 2: Exploratory Data Analysis (EDA)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"📈 EXPLORATORY DATA ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def perform_eda(df):\n",
    "    \"\"\"Perform comprehensive EDA\"\"\"\n",
    "    \n",
    "    # Basic statistics\n",
    "    print(\"📋 Basic Statistics:\")\n",
    "    print(df.describe())\n",
    "    \n",
    "    # Missing values\n",
    "    print(\"\\n🔍 Missing Values:\")\n",
    "    missing_data = df.isnull().sum()\n",
    "    missing_percent = (missing_data / len(df)) * 100\n",
    "    missing_df = pd.DataFrame({\n",
    "        'Missing Count': missing_data,\n",
    "        'Missing %': missing_percent\n",
    "    })\n",
    "    print(missing_df[missing_df['Missing Count'] > 0])\n",
    "    \n",
    "    # Categorical variables distribution\n",
    "    print(\"\\n📊 Categorical Variables Distribution:\")\n",
    "    cat_cols = ['category', 'device', 'country']\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "    \n",
    "    for i, col in enumerate(cat_cols):\n",
    "        df[col].value_counts().plot(kind='bar', ax=axes[i])\n",
    "        axes[i].set_title(f'Distribution of {col}')\n",
    "        axes[i].tick_params(axis='x', rotation=45)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Numerical variables correlation\n",
    "    print(\"\\n🔗 Correlation Heatmap:\")\n",
    "    num_cols = ['views', 'likes', 'comments', 'watch_time_minutes', 'video_length_minutes', \n",
    "                'subscribers', 'ad_revenue_usd']\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    correlation_matrix = df[num_cols].corr()\n",
    "    sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', center=0)\n",
    "    plt.title('Correlation Matrix of Numerical Features')\n",
    "    plt.show()\n",
    "    \n",
    "    # Revenue distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(df['ad_revenue_usd'], bins=30, alpha=0.7, color='skyblue')\n",
    "    plt.title('Distribution of Ad Revenue')\n",
    "    plt.xlabel('Revenue (USD)')\n",
    "    \n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.scatter(df['views'], df['ad_revenue_usd'], alpha=0.5)\n",
    "    plt.title('Views vs Revenue')\n",
    "    plt.xlabel('Views')\n",
    "    plt.ylabel('Revenue (USD)')\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.scatter(df['watch_time_minutes'], df['ad_revenue_usd'], alpha=0.5)\n",
    "    plt.title('Watch Time vs Revenue')\n",
    "    plt.xlabel('Watch Time (minutes)')\n",
    "    plt.ylabel('Revenue (USD)')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return missing_df\n",
    "\n",
    "# Perform EDA\n",
    "missing_info = perform_eda(df)\n",
    "\n",
    "# Step 3: Data Preprocessing\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🧹 DATA PREPROCESSING\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Clean and preprocess the dataset\"\"\"\n",
    "    \n",
    "    # Remove duplicates\n",
    "    print(f\"Original dataset: {df.shape}\")\n",
    "    df_clean = df.drop_duplicates()\n",
    "    print(f\"After removing duplicates: {df_clean.shape}\")\n",
    "    \n",
    "    # Handle missing values\n",
    "    print(f\"\\nMissing values before imputation:\")\n",
    "    print(df_clean.isnull().sum())\n",
    "    \n",
    "    # Impute numerical missing values with median\n",
    "    num_cols = ['likes', 'comments']\n",
    "    imputer = SimpleImputer(strategy='median')\n",
    "    df_clean[num_cols] = imputer.fit_transform(df_clean[num_cols])\n",
    "    \n",
    "    print(f\"Missing values after imputation:\")\n",
    "    print(df_clean.isnull().sum())\n",
    "    \n",
    "    # Feature Engineering\n",
    "    print(\"\\n🔧 Feature Engineering:\")\n",
    "    \n",
    "    # Engagement rate\n",
    "    df_clean['engagement_rate'] = (df_clean['likes'] + df_clean['comments']) / df_clean['views']\n",
    "    \n",
    "    # Views per subscriber\n",
    "    df_clean['views_per_subscriber'] = df_clean['views'] / df_clean['subscribers']\n",
    "    \n",
    "    # Watch time per view\n",
    "    df_clean['watch_time_per_view'] = df_clean['watch_time_minutes'] / df_clean['views']\n",
    "    \n",
    "    # Video completion rate\n",
    "    df_clean['completion_rate'] = df_clean['watch_time_minutes'] / (df_clean['views'] * df_clean['video_length_minutes'])\n",
    "    \n",
    "    print(\"✅ New features created:\")\n",
    "    print(\"- engagement_rate\")\n",
    "    print(\"- views_per_subscriber\")\n",
    "    print(\"- watch_time_per_view\")\n",
    "    print(\"- completion_rate\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Preprocess data\n",
    "df_processed = preprocess_data(df)\n",
    "print(f\"\\nFinal processed dataset: {df_processed.shape}\")\n",
    "\n",
    "# Step 4: Prepare data for modeling\n",
    "def prepare_model_data(df):\n",
    "    \"\"\"Prepare features and target for modeling\"\"\"\n",
    "    \n",
    "    # Select features\n",
    "    feature_cols = [\n",
    "        'views', 'likes', 'comments', 'watch_time_minutes', 'video_length_minutes',\n",
    "        'subscribers', 'engagement_rate', 'views_per_subscriber', \n",
    "        'watch_time_per_view', 'completion_rate'\n",
    "    ]\n",
    "    \n",
    "    # Encode categorical variables\n",
    "    le_category = LabelEncoder()\n",
    "    le_device = LabelEncoder()\n",
    "    le_country = LabelEncoder()\n",
    "    \n",
    "    df_encoded = df.copy()\n",
    "    df_encoded['category_encoded'] = le_category.fit_transform(df['category'])\n",
    "    df_encoded['device_encoded'] = le_device.fit_transform(df['device'])\n",
    "    df_encoded['country_encoded'] = le_country.fit_transform(df['country'])\n",
    "    \n",
    "    # Add encoded features to feature list\n",
    "    feature_cols.extend(['category_encoded', 'device_encoded', 'country_encoded'])\n",
    "    \n",
    "    # Prepare X and y\n",
    "    X = df_encoded[feature_cols]\n",
    "    y = df_encoded['ad_revenue_usd']\n",
    "    \n",
    "    # FIXED: Remove NaN values before scaling (ONE LINE ADDED)\n",
    "    df_clean = df_encoded.dropna()\n",
    "    X = X.loc[df_clean.index]\n",
    "    y = y.loc[df_clean.index]\n",
    "    \n",
    "    # Scale features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X_scaled, y, test_size=0.2, random_state=42\n",
    "    )\n",
    "    \n",
    "    print(f\"Training set: {X_train.shape}\")\n",
    "    print(f\"Test set: {X_test.shape}\")\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test, scaler, feature_cols, {\n",
    "        'category': le_category, 'device': le_device, 'country': le_country\n",
    "    }\n",
    "\n",
    "# Prepare data\n",
    "X_train, X_test, y_train, y_test, scaler, feature_cols, encoders = prepare_model_data(df_processed)\n",
    "\n",
    "# Step 5: Model Building and Evaluation\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🤖 MODEL BUILDING & EVALUATION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def build_and_evaluate_models(X_train, X_test, y_train, y_test):\n",
    "    \"\"\"Build and evaluate multiple regression models\"\"\"\n",
    "    \n",
    "    # Define models\n",
    "    models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Decision Tree': DecisionTreeRegressor(random_state=42),\n",
    "        'Random Forest': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'SVR': SVR(kernel='rbf'),\n",
    "        'KNN': KNeighborsRegressor(n_neighbors=5)\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    \n",
    "    print(\"Training and evaluating models...\")\n",
    "    print(\"-\" * 40)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        # Train model\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        # Make predictions\n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        r2 = r2_score(y_test, y_pred)\n",
    "        rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "        mae = mean_absolute_error(y_test, y_pred)\n",
    "        \n",
    "        results[name] = {\n",
    "            'model': model,\n",
    "            'r2': r2,\n",
    "            'rmse': rmse,\n",
    "            'mae': mae\n",
    "        }\n",
    "        \n",
    "        print(f\"{name}:\")\n",
    "        print(f\"  R² Score: {r2:.4f}\")\n",
    "        print(f\"  RMSE: {rmse:.4f}\")\n",
    "        print(f\"  MAE: {mae:.4f}\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Build models\n",
    "model_results = build_and_evaluate_models(X_train, X_test, y_train, y_test)\n",
    "\n",
    "# Step 6: Model Comparison\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🏆 MODEL COMPARISON\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def compare_models(results):\n",
    "    \"\"\"Compare model performance\"\"\"\n",
    "    \n",
    "    # Create comparison dataframe\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Model': list(results.keys()),\n",
    "        'R² Score': [results[model]['r2'] for model in results.keys()],\n",
    "        'RMSE': [results[model]['rmse'] for model in results.keys()],\n",
    "        'MAE': [results[model]['mae'] for model in results.keys()]\n",
    "    })\n",
    "    \n",
    "    # Sort by R² score\n",
    "    comparison_df = comparison_df.sort_values('R² Score', ascending=False)\n",
    "    \n",
    "    print(\"Model Performance Comparison:\")\n",
    "    print(comparison_df.round(4))\n",
    "    \n",
    "    # Visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(15, 5))\n",
    "    \n",
    "    # R² scores\n",
    "    ax1.barh(comparison_df['Model'], comparison_df['R² Score'], color='skyblue')\n",
    "    ax1.set_xlabel('R² Score')\n",
    "    ax1.set_title('Model Comparison - R² Score')\n",
    "    ax1.invert_yaxis()\n",
    "    \n",
    "    # RMSE\n",
    "    ax2.barh(comparison_df['Model'], comparison_df['RMSE'], color='salmon')\n",
    "    ax2.set_xlabel('RMSE')\n",
    "    ax2.set_title('Model Comparison - RMSE')\n",
    "    ax2.invert_yaxis()\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return comparison_df\n",
    "\n",
    "# Compare models\n",
    "comparison_df = compare_models(model_results)\n",
    "\n",
    "# Step 7: Feature Importance Analysis (for Random Forest)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🔍 FEATURE IMPORTANCE ANALYSIS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def analyze_feature_importance(model_results, feature_cols):\n",
    "    \"\"\"Analyze feature importance using Random Forest\"\"\"\n",
    "    \n",
    "    rf_model = model_results['Random Forest']['model']\n",
    "    importances = rf_model.feature_importances_\n",
    "    \n",
    "    # Create feature importance dataframe\n",
    "    importance_df = pd.DataFrame({\n",
    "        'Feature': feature_cols,\n",
    "        'Importance': importances\n",
    "    }).sort_values('Importance', ascending=False)\n",
    "    \n",
    "    print(\"Top 10 Most Important Features:\")\n",
    "    print(importance_df.head(10))\n",
    "    \n",
    "    # Plot feature importance\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    top_features = importance_df.head(10)\n",
    "    plt.barh(range(len(top_features)), top_features['Importance'])\n",
    "    plt.yticks(range(len(top_features)), top_features['Feature'])\n",
    "    plt.xlabel('Feature Importance')\n",
    "    plt.title('Top 10 Feature Importances (Random Forest)')\n",
    "    plt.gca().invert_yaxis()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return importance_df\n",
    "\n",
    "# Analyze features\n",
    "importance_df = analyze_feature_importance(model_results, feature_cols)\n",
    "\n",
    "# Step 8: Final Model Selection\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"🎯 FINAL MODEL SELECTION\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "best_model_name = comparison_df.iloc[0]['Model']\n",
    "best_model = model_results[best_model_name]['model']\n",
    "best_r2 = comparison_df.iloc[0]['R² Score']\n",
    "\n",
    "print(f\"🏆 Best Model: {best_model_name}\")\n",
    "print(f\"📊 R² Score: {best_r2:.4f}\")\n",
    "print(f\"📏 RMSE: {comparison_df.iloc[0]['RMSE']:.4f}\")\n",
    "print(f\"📐 MAE: {comparison_df.iloc[0]['MAE']:.4f}\")\n",
    "\n",
    "# Step 9: Insights and Recommendations\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"💡 BUSINESS INSIGHTS & RECOMMENDATIONS\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "def generate_insights(importance_df, df_processed):\n",
    "    \"\"\"Generate business insights\"\"\"\n",
    "    \n",
    "    print(\"🎯 KEY INSIGHTS:\")\n",
    "    print()\n",
    "    \n",
    "    top_features = importance_df.head(5)\n",
    "    \n",
    "    print(\"1. MOST IMPORTANT REVENUE DRIVERS:\")\n",
    "    for i, row in top_features.iterrows():\n",
    "        feature = row['Feature']\n",
    "        importance = row['Importance']\n",
    "        print(f\"   • {feature}: {importance:.3f} importance\")\n",
    "    \n",
    "    print(\"\\n2. BUSINESS RECOMMENDATIONS:\")\n",
    "    print(\"   • Focus on maximizing watch time - it's the #1 revenue driver\")\n",
    "    print(\"   • Higher engagement rates (likes + comments) significantly boost revenue\")\n",
    "    print(\"   • Longer videos tend to generate more revenue (optimize content length)\")\n",
    "    print(\"   • Target high-view content categories like Entertainment and Gaming\")\n",
    "    print(\"   • Mobile users generate more revenue - optimize for mobile experience\")\n",
    "    \n",
    "    print(\"\\n3. CONTENT STRATEGY TIPS:\")\n",
    "    print(\"   • Aim for 10-15 minute videos for optimal engagement\")\n",
    "    print(\"   • Create content that encourages comments and likes\")\n",
    "    print(\"   • Focus on trending topics in high-revenue categories\")\n",
    "    print(\"   • Maintain consistent upload schedule to build watch time\")\n",
    "    \n",
    "    # Revenue by category\n",
    "    print(\"\\n4. CATEGORY PERFORMANCE:\")\n",
    "    category_revenue = df_processed.groupby('category')['ad_revenue_usd'].agg(['mean', 'count']).round(2)\n",
    "    print(category_revenue.sort_values('mean', ascending=False))\n",
    "    \n",
    "    return top_features\n",
    "\n",
    "# Generate insights\n",
    "top_features = generate_insights(importance_df, df_processed)\n",
    "\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "print(\"✅ PROJECT COMPLETED SUCCESSFULLY!\")\n",
    "print(\"=\"*50)\n",
    "print(\"📁 Deliverables Created:\")\n",
    "print(\"   • Cleaned and processed dataset\")\n",
    "print(\"   • 5 trained regression models with evaluation\")\n",
    "print(\"   • Feature importance analysis\")\n",
    "print(\"   • Business insights and recommendations\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
